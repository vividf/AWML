# Unified Pipeline Architecture Implementation

## Phase 1: åŸºç¤Žæž¶æ§‹ âœ… Completed

### 1. Base Pipeline æž¶æ§‹

å·²å‰µå»ºçš„æ–‡ä»¶:
- `autoware_ml/deployment/pipelines/base_pipeline.py` âœ…
- `autoware_ml/deployment/pipelines/detection_2d_pipeline.py` âœ…
- `autoware_ml/deployment/pipelines/detection_3d_pipeline.py` âœ…
- `autoware_ml/deployment/pipelines/classification_pipeline.py` âœ…

### 2. CenterPoint é‡æ§‹ âœ…

å·²æ›´æ–°:
- `centerpoint_pipeline.py` - ç¹¼æ‰¿è‡ª `Detection3DPipeline`
- `centerpoint_pytorch.py` - å‚³éž `backend_type="pytorch"`
- `centerpoint_onnx.py` - å‚³éž `backend_type="onnx"`
- `centerpoint_tensorrt.py` - å‚³éž `backend_type="tensorrt"`

### 3. çµ±ä¸€æž¶æ§‹åœ–

```
BaseDeploymentPipeline (abstract)
â”œâ”€â”€ Detection2DPipeline
â”‚   â””â”€â”€ YOLOXDeploymentPipeline
â”‚       â”œâ”€â”€ YOLOXPyTorchPipeline
â”‚       â”œâ”€â”€ YOLOXONNXPipeline
â”‚       â””â”€â”€ YOLOXTensorRTPipeline
â”‚
â”œâ”€â”€ Detection3DPipeline
â”‚   â””â”€â”€ CenterPointDeploymentPipeline
â”‚       â”œâ”€â”€ CenterPointPyTorchPipeline âœ…
â”‚       â”œâ”€â”€ CenterPointONNXPipeline âœ…
â”‚       â””â”€â”€ CenterPointTensorRTPipeline âœ…
â”‚
â””â”€â”€ ClassificationPipeline
    â””â”€â”€ CalibrationPipeline
        â”œâ”€â”€ CalibrationPyTorchPipeline
        â””â”€â”€ CalibrationONNXPipeline
```

## Phase 2: YOLOX-ELAN é·ç§»

### ç›®éŒ„çµæ§‹

```
autoware_ml/deployment/pipelines/yolox/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ yolox_pipeline.py          # YOLOXDeploymentPipeline (base)
â”œâ”€â”€ yolox_pytorch.py            # YOLOXPyTorchPipeline
â”œâ”€â”€ yolox_onnx.py               # YOLOXONNXPipeline
â””â”€â”€ yolox_tensorrt.py           # YOLOXTensorRTPipeline
```

### 1. YOLOX Pipeline åŸºé¡ž

```python
# autoware_ml/deployment/pipelines/yolox/yolox_pipeline.py

from abc import abstractmethod
from typing import List, Dict, Tuple
import numpy as np
import torch

from ..detection_2d_pipeline import Detection2DPipeline


class YOLOXDeploymentPipeline(Detection2DPipeline):
    """
    Base class for YOLOX deployment across different backends.
    
    YOLOX-specific features:
    - Multi-scale feature fusion
    - Anchor-free detection
    - SimOTA label assignment (training)
    - Decoupled head
    """
    
    def __init__(
        self, 
        model,
        device: str = "cpu",
        num_classes: int = 80,
        class_names: List[str] = None,
        input_size: Tuple[int, int] = (640, 640),
        conf_threshold: float = 0.01,
        nms_threshold: float = 0.65,
        backend_type: str = "unknown"
    ):
        """
        Initialize YOLOX pipeline.
        
        Args:
            model: Model object
            device: Device for inference
            num_classes: Number of classes (COCO=80, T4Dataset=5)
            class_names: List of class names
            input_size: Model input size (height, width)
            conf_threshold: Confidence threshold
            nms_threshold: NMS IoU threshold
            backend_type: Backend type
        """
        super().__init__(
            model=model,
            device=device,
            num_classes=num_classes,
            class_names=class_names,
            input_size=input_size,
            backend_type=backend_type
        )
        
        self.conf_threshold = conf_threshold
        self.nms_threshold = nms_threshold
    
    def preprocess(self, image: np.ndarray, **kwargs) -> Tuple[torch.Tensor, Dict]:
        """
        YOLOX-specific preprocessing.
        
        Uses letterbox resizing (padding to maintain aspect ratio).
        """
        # Call parent preprocess
        tensor, metadata = super().preprocess(image, **kwargs)
        
        return tensor, metadata
    
    @abstractmethod
    def run_model(self, preprocessed_input: torch.Tensor):
        """Run YOLOX model (backend-specific)."""
        pass
    
    def postprocess(
        self, 
        model_output: torch.Tensor,
        metadata: Dict = None
    ) -> List[Dict]:
        """
        YOLOX-specific postprocessing.
        
        Args:
            model_output: Model output [1, num_predictions, 85] 
                         where 85 = 4(bbox) + 1(obj_conf) + 80(class_probs)
            metadata: Preprocessing metadata
            
        Returns:
            List of detections
        """
        if isinstance(model_output, torch.Tensor):
            predictions = model_output.cpu().numpy()
        else:
            predictions = model_output
        
        # Remove batch dimension
        if predictions.ndim == 3:
            predictions = predictions[0]  # [num_predictions, 85]
        
        # Extract components
        boxes = predictions[:, :4]        # [x_center, y_center, w, h]
        obj_conf = predictions[:, 4]      # objectness confidence
        class_probs = predictions[:, 5:]  # class probabilities
        
        # Convert to [x1, y1, x2, y2]
        x1 = boxes[:, 0] - boxes[:, 2] / 2
        y1 = boxes[:, 1] - boxes[:, 3] / 2
        x2 = boxes[:, 0] + boxes[:, 2] / 2
        y2 = boxes[:, 1] + boxes[:, 3] / 2
        boxes = np.stack([x1, y1, x2, y2], axis=1)
        
        # Get class with highest probability
        class_ids = np.argmax(class_probs, axis=1)
        class_confidences = class_probs[np.arange(len(class_probs)), class_ids]
        
        # Final confidence = objectness * class_confidence
        confidences = obj_conf * class_confidences
        
        # Filter by confidence threshold
        mask = confidences >= self.conf_threshold
        boxes = boxes[mask]
        confidences = confidences[mask]
        class_ids = class_ids[mask]
        
        # Transform coordinates back to original image space
        if metadata:
            boxes = self._transform_coordinates(
                boxes,
                metadata['scale'],
                metadata['pad'],
                metadata['original_shape']
            )
        
        # Apply NMS per class
        detections = []
        for class_id in np.unique(class_ids):
            class_mask = class_ids == class_id
            class_boxes = boxes[class_mask]
            class_confidences = confidences[class_mask]
            
            # NMS
            keep_indices = self._nms(class_boxes, class_confidences, self.nms_threshold)
            
            for idx in keep_indices:
                detections.append({
                    'bbox': class_boxes[idx].tolist(),
                    'score': float(class_confidences[idx]),
                    'class_id': int(class_id),
                    'class_name': self.class_names[class_id] if class_id < len(self.class_names) else f"class_{class_id}"
                })
        
        return detections
```

### 2. YOLOX PyTorch Pipeline

```python
# autoware_ml/deployment/pipelines/yolox/yolox_pytorch.py

import torch
from .yolox_pipeline import YOLOXDeploymentPipeline


class YOLOXPyTorchPipeline(YOLOXDeploymentPipeline):
    """PyTorch implementation of YOLOX pipeline."""
    
    def __init__(self, model, device: str = "cpu", **kwargs):
        super().__init__(model, device, backend_type="pytorch", **kwargs)
        self.model.eval()
    
    def run_model(self, preprocessed_input: torch.Tensor):
        """Run PyTorch YOLOX model."""
        with torch.no_grad():
            outputs = self.model(preprocessed_input)
        return outputs
```

### 3. YOLOX ONNX Pipeline

```python
# autoware_ml/deployment/pipelines/yolox/yolox_onnx.py

import onnxruntime as ort
import torch
import numpy as np

from .yolox_pipeline import YOLOXDeploymentPipeline


class YOLOXONNXPipeline(YOLOXDeploymentPipeline):
    """ONNX Runtime implementation of YOLOX pipeline."""
    
    def __init__(self, model, onnx_path: str, device: str = "cpu", **kwargs):
        super().__init__(model, device, backend_type="onnx", **kwargs)
        
        # Create ONNX session
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device.startswith("cuda") else ['CPUExecutionProvider']
        self.session = ort.InferenceSession(onnx_path, providers=providers)
        
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
    
    def run_model(self, preprocessed_input: torch.Tensor):
        """Run ONNX YOLOX model."""
        # Convert to numpy
        input_array = preprocessed_input.cpu().numpy().astype(np.float32)
        
        # Run ONNX inference
        outputs = self.session.run(self.output_names, {self.input_name: input_array})
        
        # Convert back to torch
        return torch.from_numpy(outputs[0]).to(self.device)
```

### 4. YOLOX TensorRT Pipeline

```python
# autoware_ml/deployment/pipelines/yolox/yolox_tensorrt.py

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import torch
import numpy as np

from .yolox_pipeline import YOLOXDeploymentPipeline


class YOLOXTensorRTPipeline(YOLOXDeploymentPipeline):
    """TensorRT implementation of YOLOX pipeline."""
    
    def __init__(self, model, engine_path: str, device: str = "cuda", **kwargs):
        if not device.startswith("cuda"):
            raise ValueError("TensorRT requires CUDA device")
        
        super().__init__(model, device, backend_type="tensorrt", **kwargs)
        
        # Load TensorRT engine
        self.logger = trt.Logger(trt.Logger.WARNING)
        trt.init_libnvinfer_plugins(self.logger, "")
        
        with open(engine_path, "rb") as f:
            runtime = trt.Runtime(self.logger)
            self.engine = runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
    
    def run_model(self, preprocessed_input: torch.Tensor):
        """Run TensorRT YOLOX model."""
        input_array = preprocessed_input.cpu().numpy().astype(np.float32)
        
        # Get input/output names
        input_name = self.engine.get_tensor_name(0)
        output_name = self.engine.get_tensor_name(1)
        
        # Set input shape
        self.context.set_input_shape(input_name, input_array.shape)
        
        # Get output shape
        output_shape = self.context.get_tensor_shape(output_name)
        output_array = np.empty(output_shape, dtype=np.float32)
        
        # Allocate GPU memory
        d_input = cuda.mem_alloc(input_array.nbytes)
        d_output = cuda.mem_alloc(output_array.nbytes)
        
        # Create CUDA stream
        stream = cuda.Stream()
        
        try:
            # Set tensor addresses
            self.context.set_tensor_address(input_name, int(d_input))
            self.context.set_tensor_address(output_name, int(d_output))
            
            # Run inference
            cuda.memcpy_htod_async(d_input, input_array, stream)
            self.context.execute_async_v3(stream_handle=stream.handle)
            cuda.memcpy_dtoh_async(output_array, d_output, stream)
            stream.synchronize()
            
            return torch.from_numpy(output_array).to(self.device)
            
        finally:
            d_input.free()
            d_output.free()
```

### 5. __init__.py

```python
# autoware_ml/deployment/pipelines/yolox/__init__.py

from .yolox_pipeline import YOLOXDeploymentPipeline
from .yolox_pytorch import YOLOXPyTorchPipeline
from .yolox_onnx import YOLOXONNXPipeline
from .yolox_tensorrt import YOLOXTensorRTPipeline

__all__ = [
    'YOLOXDeploymentPipeline',
    'YOLOXPyTorchPipeline',
    'YOLOXONNXPipeline',
    'YOLOXTensorRTPipeline',
]
```

## ä½¿ç”¨ç¤ºä¾‹

### CenterPoint (Already Working)

```python
from autoware_ml.deployment.pipelines import (
    CenterPointPyTorchPipeline,
    CenterPointONNXPipeline,
    CenterPointTensorRTPipeline
)

# PyTorch
pytorch_pipeline = CenterPointPyTorchPipeline(model, device="cuda")
predictions, latency = pytorch_pipeline.infer(points)

# ONNX
onnx_pipeline = CenterPointONNXPipeline(model, onnx_dir="path/to/onnx", device="cpu")
predictions, latency = onnx_pipeline.infer(points)

# Verification
raw_pytorch, _ = pytorch_pipeline.infer(points, return_raw_outputs=True)
raw_onnx, _ = onnx_pipeline.infer(points, return_raw_outputs=True)
# Compare raw_pytorch and raw_onnx
```

### YOLOX (New)

```python
from autoware_ml.deployment.pipelines.yolox import (
    YOLOXPyTorchPipeline,
    YOLOXONNXPipeline,
    YOLOXTensorRTPipeline
)

# PyTorch
pytorch_pipeline = YOLOXPyTorchPipeline(model, device="cuda")
detections, latency = pytorch_pipeline.infer(image)

# ONNX
onnx_pipeline = YOLOXONNXPipeline(model, onnx_path="yolox.onnx", device="cpu")
detections, latency = onnx_pipeline.infer(image)

# TensorRT
trt_pipeline = YOLOXTensorRTPipeline(model, engine_path="yolox.engine", device="cuda")
detections, latency = trt_pipeline.infer(image)
```

## çµ±ä¸€ Evaluator (ç°¡åŒ–ç‰ˆæœ¬)

```python
# autoware_ml/deployment/evaluators/unified_evaluator.py

from typing import List, Dict
from ..pipelines.base_pipeline import BaseDeploymentPipeline


class UnifiedEvaluator:
    """Unified evaluator for all pipeline types."""
    
    def evaluate(
        self,
        pipeline: BaseDeploymentPipeline,
        data_loader,
        num_samples: int
    ) -> Dict:
        """Evaluate using any pipeline."""
        predictions_list = []
        ground_truths_list = []
        latencies = []
        
        for i in range(num_samples):
            sample = data_loader.load_sample(i)
            gt = data_loader.get_ground_truth(i)
            
            predictions, latency = pipeline.infer(sample)
            
            predictions_list.append(predictions)
            ground_truths_list.append(gt)
            latencies.append(latency)
        
        # Compute metrics based on task type
        if pipeline.task_type == "detection_2d":
            metrics = self._compute_2d_metrics(predictions_list, ground_truths_list)
        elif pipeline.task_type == "detection_3d":
            metrics = self._compute_3d_metrics(predictions_list, ground_truths_list)
        elif pipeline.task_type == "classification":
            metrics = self._compute_classification_metrics(predictions_list, ground_truths_list)
        else:
            metrics = {}
        
        metrics['latency'] = {
            'mean_ms': np.mean(latencies),
            'std_ms': np.std(latencies)
        }
        
        return metrics
    
    def verify(
        self,
        reference_pipeline: BaseDeploymentPipeline,
        target_pipeline: BaseDeploymentPipeline,
        data_loader,
        num_samples: int = 5,
        tolerance: float = 1e-3
    ) -> Dict:
        """Verify pipeline outputs."""
        results = {}
        
        for i in range(num_samples):
            sample = data_loader.load_sample(i)
            
            # Get raw outputs
            ref_out, _ = reference_pipeline.infer(sample, return_raw_outputs=True)
            target_out, _ = target_pipeline.infer(sample, return_raw_outputs=True)
            
            # Compare
            max_diff = self._compute_max_diff(ref_out, target_out)
            results[f'sample_{i}'] = max_diff < tolerance
        
        return results
```

## ä¸‹ä¸€æ­¥

1. âœ… Phase 1 å®Œæˆ - åŸºç¤Žæž¶æ§‹å»ºç«‹
2. ðŸ“ Phase 2 è¦åŠƒå®Œæˆ - YOLOXå¯¦ç¾ç´°ç¯€å·²æä¾›
3. â³ Phase 3 å¾…åŸ·è¡Œ - Calibrationé·ç§»
4. â³ Phase 4 å¾…åŸ·è¡Œ - æ¸…ç†èˆŠä»£ç¢¼

## é—œéµå„ªå‹¢

1. **ä»£ç¢¼å¾©ç”¨**: å‰å¾Œè™•ç†å…±äº«ï¼Œæ¸›å°‘é‡è¤‡ä»£ç¢¼ 40-50%
2. **ä¸€è‡´æ€§**: æ‰€æœ‰ backend ä½¿ç”¨ç›¸åŒé‚è¼¯
3. **å¯æ“´å±•**: æ·»åŠ æ–°æ¨¡åž‹åªéœ€ç¹¼æ‰¿åŸºé¡ž
4. **æ˜“æ¸¬è©¦**: çµ±ä¸€æŽ¥å£ä¾¿æ–¼å–®å…ƒæ¸¬è©¦
5. **æ˜“ç¶­è­·**: ä¿®å¾© bug ä¸€æ¬¡å³å¯æ‡‰ç”¨åˆ°æ‰€æœ‰æ¨¡åž‹

## å¯¦éš›éƒ¨ç½²æ•ˆçŽ‡æå‡

- **é–‹ç™¼æ™‚é–“**: å¾ž 3-5 å¤©æ¸›å°‘åˆ° 1-2 å¤©
- **ä»£ç¢¼é‡**: æ¸›å°‘ 40-50%
- **Bugä¿®å¾©**: ä¸€æ¬¡ä¿®å¾©å…¨å±€å—ç›Š
- **æ–°backend**: å¾žå¹¾å¤©æ¸›å°‘åˆ°å¹¾å°æ™‚

