

import os
import time
import pickle
import json
from typing import Any, Dict, List, Optional, Sequence

# Third-party Libraries
import numpy as np
from pyquaternion import Quaternion
from mmengine.evaluator import BaseMetric
from mmengine.logging import MMLogger
from mmdet3d.registry import METRICS

# TIER IV Perception Evaluation Library
from perception_eval.config import PerceptionEvaluationConfig
from perception_eval.manager import PerceptionEvaluationManager
from perception_eval.evaluation.metrics import MetricsScoreConfig
from perception_eval.evaluation.result.perception_frame_config import (
    CriticalObjectFilterConfig,
    PerceptionPassFailConfig,
)
from perception_eval.evaluation.result.perception_frame_result import PerceptionFrameResult
from perception_eval.evaluation.result.object_result import DynamicObjectWithPerceptionResult

from perception_eval.common.dataset import FrameGroundTruth
from perception_eval.common.object import DynamicObject
from perception_eval.common.label import Label, AutowareLabel
from perception_eval.common.evaluation_task import EvaluationTask
from perception_eval.common.shape import Shape, ShapeType



__all__ = ["T4Metric"]


@METRICS.register_module()
class T4Metric(BaseMetric):
    """T4 format evaluation metric.
    Args:
        data_root (str):
            Path of dataset root.
        ann_file (str):
            Path of annotation file.
        prefix (str, optional):
            The prefix that will be added in the metric
            names to disambiguate homonymous metrics of different evaluators.
            If prefix is not provided in the argument, self.default_prefix will
            be used instead. Defaults to None.
        collect_device (str):
            Device name used for collecting results from
            different ranks during distributed training. Must be 'cpu' or 'gpu'.
            Defaults to 'cpu'.
        class_names (List[str], optional):
            The class names. Defaults to [].
        name_mapping (dict, optional):
            The data class mapping, applied to ground truth during evaluation.
            Defaults to None.
        version (str, optional):
            The version of the dataset. Defaults to "".
    """



    def __init__(
        self,
        data_root: str,
        ann_file: str,
        prefix: Optional[str] = None,
        collect_device: str = "cpu",
        class_names: List[str] = [],
        name_mapping: Optional[dict] = None,
        perception_evaluator_configs: Dict[str, Any] = {},
        critical_object_filter_config: Dict[str, Any] = {},
        perception_pass_fail_config: Dict[str, Any] = {},
        save_prediction_result_to_pickle: bool = False,
        load_prediction_result_from_pickle: bool = False,
        pickle_path: Optional[str] = None
    ) -> None:
       
       
        self.default_prefix = 'T4 metric'
        super(T4Metric, self).__init__(
            collect_device=collect_device, prefix=prefix)
        self.ann_file = ann_file
        self.data_root = data_root
    
        self.class_names = class_names
        self.name_mapping = name_mapping
        
        if name_mapping is not None:
            self.class_names = [self.name_mapping.get(name, name) for name in self.class_names]
        
        self.save_prediction_result_to_pickle = save_prediction_result_to_pickle
        self.load_prediction_result_from_pickle = load_prediction_result_from_pickle
        self.pickle_path = pickle_path

        self.perception_evaluator_configs : PerceptionEvaluationConfig = PerceptionEvaluationConfig(**perception_evaluator_configs)

        self.critical_object_filter_config : CriticalObjectFilterConfig = CriticalObjectFilterConfig(
            evaluator_config=self.perception_evaluator_configs, **critical_object_filter_config
        )
        self.frame_pass_fail_config: PerceptionPassFailConfig = PerceptionPassFailConfig(
            evaluator_config=self.perception_evaluator_configs, **perception_pass_fail_config
        )

    def get_label_from_model_output(self, model_output: int) -> Label:
        """
        Retrieves a label from the model's output index

        Args:
            model_output (int): The index output by the model.

        Returns:
            Label: A Label object containing the corresponding AutowareLabel 
                and the original class name.
        """
        # Check if the model output index is within the valid range
        name = self.class_names[model_output] if 0 <= model_output < len(self.class_names) else "unknown"
        autoware_label = AutowareLabel.__members__.get(name.upper(), AutowareLabel.UNKNOWN)
        return Label(label=autoware_label, name=name)


    def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
        """Process one batch of data samples and predictions.

        The processed results should be stored in ``self.results``, which will
        be used to compute the metrics when all batches have been processed.

        Args:
            data_batch (dict): A batch of data from the dataloader.
            data_samples (Sequence[dict]): A batch of outputs from the model and the ground truth of dataset
        """

        if self.load_prediction_result_from_pickle:
            return


        for data_sample in data_samples:
            current_time = time.time()
            scene_id = self.parse_scene_id(data_sample["lidar_path"])
            frame_ground_truth = self.parse_ground_truth_from_sample(current_time, data_sample)
            perception_frame_result = self.parse_predictions_from_sample(current_time, data_sample, frame_ground_truth)
            self.save_perception_results(scene_id, data_sample['sample_idx'], perception_frame_result)
        
        if self.save_prediction_result_to_pickle:
            self.save_results_to_pickle(self.pickle_path)


    def parse_scene_id(self, lidar_path: str) -> str:
        """parse scene ID from the LiDAR file path.

        Removes the `data_root` prefix and the trailing `/data` section.

        Args:
            lidar_path (str): The full file path of the LiDAR data.

        Returns:
            str: The extracted scene ID, or "unknown" if extraction fails.
        """
        if not lidar_path or not lidar_path.startswith(self.data_root):
            return "unknown"

        # Remove the data_root prefix
        relative_path = lidar_path[len(self.data_root):].lstrip("/")  # Remove leading slash if exists
        path_parts = relative_path.split("/")

        # Extract scene ID before "data" section
        try:
            data_index = path_parts.index("data")
            return "/".join(path_parts[:data_index])
        except ValueError:
            return "unknown"
    

    def parse_ground_truth_from_sample(self, time: float, data_sample: Dict) -> FrameGroundTruth:
        """Parses ground truth objects from the given data sample.

        Args:
            time (float): The timestamp (in seconds) of the frame.
            data_sample (Dict): A dictionary containing ground truth annotations,
                                including 3D bounding boxes, labels, and LiDAR point counts.

        Returns:
            FrameGroundTruth: A structured representation of the ground truth objects,
                            including position, orientation, shape, velocity, and labels.
        """
        eval_info = data_sample.get("eval_ann_info", {})
        sample_id = data_sample.get('sample_idx', "unknown")
        
        gt_bboxes_3d = eval_info.get("gt_bboxes_3d", [])
        gt_labels_3d = eval_info.get("gt_labels_3d", [])
        num_lidar_pts = eval_info.get("num_lidar_pts", [])

        bboxes = (bbox.tolist() for bbox in gt_bboxes_3d.tensor.cpu().numpy())

        objects = (
            DynamicObject(
                unix_time=time,  
                frame_id=self.perception_evaluator_configs.frame_id,
                position=tuple(bbox[:3]),  
                orientation=Quaternion(0, 0, np.sin(bbox[6] / 2), np.cos(bbox[6] / 2)), 
                shape=Shape(shape_type=ShapeType.BOUNDING_BOX, size=tuple(bbox[3:6])),
                velocity=(bbox[7], bbox[8], 0.0),
                semantic_score=1.0,
                semantic_label=self.get_label_from_model_output(int(label)),
                pointcloud_num=int(num_pts),
            )
            for bbox, label, num_pts in zip(bboxes, gt_labels_3d, num_lidar_pts)
        )

        return FrameGroundTruth(
            unix_time=time,
            frame_name=str(sample_id),
            objects=list(objects),
            transforms=None, 
            raw_data=None,
        )

    def parse_predictions_from_sample(self, time: float, data_sample: Dict, frame_ground_truth: FrameGroundTruth) -> PerceptionFrameResult:
        """
        Parses predicted objects from the data sample and creates a perception frame result.

        Args:
            time (float): The timestamp associated with the predictions.
            data_sample (Dict): A dictionary containing the predicted instances, including 3D bounding boxes, scores, and labels.
            frame_ground_truth (FrameGroundTruth): The ground truth data corresponding to the current frame.

        Returns:
            PerceptionFrameResult: A structured result containing the predicted objects, frame ground truth, and evaluation configurations.
        """
        pred_3d = data_sample.get("pred_instances_3d", {})

        bboxes = pred_3d.get("bboxes_3d", {}).tensor.cpu().numpy()
        scores = pred_3d.get("scores_3d", [])
        labels = pred_3d.get("labels_3d", [])

        # List comprehension with better clarity
        objects_with_perception = [
            DynamicObjectWithPerceptionResult(
                estimated_object=DynamicObject(
                    unix_time=time,
                    frame_id=self.perception_evaluator_configs.frame_id,
                    position=tuple(bbox[:3]),
                    orientation=Quaternion(0, 0, np.sin(bbox[6] / 2), np.cos(bbox[6] / 2)),  
                    shape= Shape(shape_type=ShapeType.BOUNDING_BOX, size=tuple(bbox[3:6])),
                    velocity=(bbox[7], bbox[8], 0.0),
                    semantic_score=float(score),
                    semantic_label=self.get_label_from_model_output(int(label)),
                ),
                ground_truth_object=None,
            )
            for bbox, score, label in zip(bboxes, scores, labels)
        ]

        return PerceptionFrameResult(
            object_results=objects_with_perception,
            frame_ground_truth=frame_ground_truth,
            metrics_config=MetricsScoreConfig(
                EvaluationTask.DETECTION,
                target_labels={AutowareLabel.CAR, AutowareLabel.TRUCK, AutowareLabel.BUS, AutowareLabel.BICYCLE, AutowareLabel.PEDESTRIAN},
                center_distance_thresholds=0.1,
                plane_distance_thresholds=0.1,
                iou_2d_thresholds=0.1,
                iou_3d_thresholds=0.1,
            ),
            critical_object_filter_config=self.critical_object_filter_config,
            frame_pass_fail_config=self.frame_pass_fail_config,
            unix_time=time,
            target_labels=[AutowareLabel.CAR, AutowareLabel.TRUCK, AutowareLabel.BUS, AutowareLabel.BICYCLE, AutowareLabel.PEDESTRIAN],
        )
    
    

    def save_perception_results(self, scene_id: str, sample_idx: int, perception_frame_result: PerceptionFrameResult) -> None:
        """
        Stores the processed perceptoin result in self.results following the format.
        [
            {
                <scence_id>:
                    {<sample_idx>: <PerceptionFrameResult>},
                    {<sample_idx>: <PerceptionFrameResult>},
                
            },
            {
                <scence_id>:
                    {<sample_idx>: <PerceptionFrameResult>},
                    {<sample_idx>: <PerceptionFrameResult>},
                    
            },
        ]

        Args:
            scene_id (str): The identifier for the scene to which the result belongs.
            sample_idx (int): The index of the sample within the scene.
            perception_frame_result (PerceptionFrameResult): The processed perception result for the given sample.
        """

        for scene_dict in self.results:
            if scene_id in scene_dict:
                # Append the sample to the existing scene
                scene_dict[scene_id][sample_idx] = perception_frame_result
                return 

        # If scene does not exist, create a new entry
        self.results.append({scene_id: {sample_idx: perception_frame_result}})

    def save_results_to_pickle(self, path: str) -> None:
        """Save self.results to a pickle file inside the given directory.

        Args:
            path (str): The directory path where the pickle file will be stored.
        """

        if not path:
            print("[Error] Path is not specified. Please provide a valid directory path for saving results.")
            return
        os.makedirs(path, exist_ok=True)
        file_path = os.path.join(path, "prediction_result.pkl")
        with open(file_path, "wb") as f:
            pickle.dump(self.results, f)



    def load_results_from_pickle(self, path: str) -> List[Dict]:
        """Loads self.results from a pickle file.

        Args:
            path (str, optional): The file path to load the pickle file from. Defaults to "prediction_result.pkl".

        Returns:
            List[Dict]: The deserialized results from the pickle file.
        """
    
        if not path:
            print("[Error] Path is not specified. Please provide a valid directory path for loading results.")
            return
        
        file_path = os.path.join(path, "prediction_result.pkl")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Pickle file not found: {file_path}")

        with open(file_path, "rb") as f:
            results = pickle.load(f)

        return results



    def compute_metrics(
        self,
        results: List[dict],
    ) -> Dict[str, float]:
        """Compute the metrics from processed results.

        Args:
            results (List[dict]): The processed results of each batch.

        Returns:
            Dict[str, float]: The computed metrics. The keys are the names of
            the metrics, and the values are corresponding results.
        """
        logger: MMLogger = MMLogger.get_current_instance()

        if self.load_prediction_result_from_pickle:
            logger.info("Loading prediction result from pickle...")
            results = self.load_results_from_pickle(self.pickle_path)

        evaluator = PerceptionEvaluationManager(evaluation_config=self.perception_evaluator_configs)

        def process_map_instance(map_instance, metrics_store):
            matching_mode = map_instance.matching_mode.value  # e.g., "Center Distance" or "Plane Distance"
            
            for ap in map_instance.aps:
                label = ap.target_labels[0].value  # AutowareLabel
                threshold = ap.matching_threshold_list[0]
                ap_value = ap.ap
                
                # Construct the metric key
                key = f"T4Metric/{label}_AP_{matching_mode.lower().replace(' ', '_')}_{threshold}"
                metrics_store[key] = ap_value


        scene_metrics = {}
        for scene_dict in results:
            for scene_id, samples in scene_dict.items():
                scene_metrics.setdefault(scene_id, {})
                for sample_id, frame_result in samples.items():
                    scene_metrics[scene_id].setdefault(sample_id, {})
                    if isinstance(frame_result, PerceptionFrameResult):
                        object_with_results = frame_result.object_results
                        estimated_objects = [obj.estimated_object for obj in object_with_results]
                        frame_result: PerceptionFrameResult = evaluator.add_frame_result(
                            unix_time=time.time(),
                            ground_truth_now_frame=frame_result.frame_ground_truth,
                            estimated_objects=estimated_objects,
                            critical_object_filter_config=self.critical_object_filter_config,
                            frame_pass_fail_config=self.frame_pass_fail_config,
                        )

                        for map_instance in frame_result.metrics_score.maps:
                            process_map_instance(map_instance, scene_metrics[scene_id][sample_id])

        final_metric_score = evaluator.get_scene_result()
        logger.info(f"final metrics result {final_metric_score}")

        metric_dict = {}
        aggregated_metrics = {'aggregated_metrics': {}}
        
        # Iterate over the list of maps in final_metric_score
        for map_instance in final_metric_score.maps:
            process_map_instance(map_instance, metric_dict)

            for key, value in metric_dict.items():
                label = key.split('/')[1].split('_')[0]  # Extract label name
                aggregated_metrics['aggregated_metrics'].setdefault(label, {})
                aggregated_metrics['aggregated_metrics'][label][key] = value

        logger.info(f"Metric dictionary: {metric_dict}")

        with open("scene_metrics.json", "w") as scene_file:
            json.dump(scene_metrics, scene_file, indent=4)

        with open("aggregated_metrics.json", "w") as agg_file:
            json.dump(aggregated_metrics, agg_file, indent=4)

        return metric_dict
