#!/usr/bin/env python3
"""
Test script for CenterPoint TensorRT deployment.

This script tests the TensorRT deployment pipeline without requiring
a full dataset or checkpoint.
"""

import os
import sys
import logging
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

import torch
import numpy as np

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_tensorrt_export():
    """Test TensorRT export functionality."""
    logger.info("Testing TensorRT export functionality...")
    
    dummy_onnx_path = "/tmp/dummy_model.onnx"
    
    try:
        # Skip TensorRT test if TensorRT is not available
        try:
            import tensorrt as trt
        except ImportError:
            logger.warning("‚ö†Ô∏è  TensorRT not available, skipping TensorRT export test")
            return True
        
        from autoware_ml.deployment.exporters.tensorrt_exporter import TensorRTExporter
        
        # Create a simple PyTorch model and export to ONNX
        class DummyModel(torch.nn.Module):
            def forward(self, x):
                return torch.relu(x)
        
        model = DummyModel()
        dummy_input = torch.randn(1, 3, 224, 224)
        
        # Export to ONNX
        torch.onnx.export(
            model,
            dummy_input,
            dummy_onnx_path,
            input_names=['input'],
            output_names=['output'],
            opset_version=11
        )
        
        logger.info(f"Created dummy ONNX model: {dummy_onnx_path}")
        
        # Test TensorRT exporter
        trt_config = {
            "precision_policy": "auto",
            "max_workspace_size": 1 << 30,  # 1 GB
        }
        
        exporter = TensorRTExporter(trt_config, logger)
        
        # Test export
        output_path = "/tmp/dummy_model.trt"
        success = exporter.export(
            model=None,
            sample_input=dummy_input,
            output_path=output_path,
            onnx_path=dummy_onnx_path
        )
        
        if success:
            logger.info("‚úÖ TensorRT export test PASSED")
            return True
        else:
            logger.error("‚ùå TensorRT export test FAILED")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå TensorRT export test FAILED with error: {e}")
        return False
    finally:
        # Cleanup
        for path in [dummy_onnx_path, "/tmp/dummy_model.trt"]:
            if os.path.exists(path):
                os.remove(path)

def test_centerpoint_tensorrt_backend():
    """Test CenterPoint TensorRT backend."""
    logger.info("Testing CenterPoint TensorRT backend...")
    
    dummy_trt_dir = "/tmp/dummy_centerpoint_trt"
    
    try:
        # Skip if TensorRT not available
        try:
            import tensorrt as trt
        except ImportError:
            logger.warning("‚ö†Ô∏è  TensorRT not available, skipping CenterPoint backend test")
            return True
            
        from projects.CenterPoint.deploy.centerpoint_tensorrt_backend import CenterPointTensorRTBackend
        
        # Create dummy TensorRT engines directory
        os.makedirs(dummy_trt_dir, exist_ok=True)
        
        # Create dummy engine files (empty files for testing)
        engine_files = [
            "pts_voxel_encoder.trt",
            "pts_backbone_neck_head.trt"
        ]
        
        for engine_file in engine_files:
            engine_path = os.path.join(dummy_trt_dir, engine_file)
            with open(engine_path, 'wb') as f:
                f.write(b'dummy_trt_engine')
        
        logger.info(f"Created dummy TensorRT engines in: {dummy_trt_dir}")
        
        # Test backend initialization
        backend = CenterPointTensorRTBackend(dummy_trt_dir, "cuda:0")
        
        # Test that it raises appropriate error when trying to load dummy engines
        try:
            backend.load_model()
            logger.warning("‚ö†Ô∏è  Expected error when loading dummy engines, but no error occurred")
        except Exception as e:
            logger.info(f"‚úÖ Expected error when loading dummy engines: {type(e).__name__}")
        
        logger.info("‚úÖ CenterPoint TensorRT backend test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå CenterPoint TensorRT backend test FAILED with error: {e}")
        return False
    finally:
        # Cleanup
        if os.path.exists(dummy_trt_dir):
            import shutil
            shutil.rmtree(dummy_trt_dir)

def test_deployment_config():
    """Test deployment configuration."""
    logger.info("Testing deployment configuration...")
    
    try:
        from autoware_ml.deployment.core import BaseDeploymentConfig
        
        # Test config loading
        config_dict = {
            "export": {
                "mode": "both",
                "verify": True,
                "device": "cuda:0",
                "work_dir": "test_work_dir"
            },
            "runtime_io": {
                "info_file": "test_info.pkl",
                "sample_idx": 0
            },
            "model_io": {
                "input_name": "voxels",
                "input_shape": (32, 4),
                "input_dtype": "float32"
            },
            "backend_config": {
                "common_config": {
                    "precision_policy": "auto",
                    "max_workspace_size": 2 << 30
                }
            },
            "verification": {
                "enabled": True,
                "tolerance": 1e-1,
                "num_verify_samples": 1
            }
        }
        
        config = BaseDeploymentConfig(config_dict)
        
        # Test export config
        assert config.export_config.mode == "both"
        assert config.export_config.should_export_onnx() == True
        assert config.export_config.should_export_tensorrt() == True
        
        logger.info("‚úÖ Deployment configuration test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Deployment configuration test FAILED with error: {e}")
        return False

def main():
    """Run all tests."""
    logger.info("=" * 80)
    logger.info("CenterPoint TensorRT Deployment Tests")
    logger.info("=" * 80)
    
    tests = [
        ("TensorRT Export", test_tensorrt_export),
        ("CenterPoint TensorRT Backend", test_centerpoint_tensorrt_backend),
        ("Deployment Configuration", test_deployment_config),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nRunning {test_name} test...")
        if test_func():
            passed += 1
        else:
            logger.error(f"{test_name} test failed!")
    
    logger.info("\n" + "=" * 80)
    logger.info(f"Test Results: {passed}/{total} tests passed")
    logger.info("=" * 80)
    
    if passed == total:
        logger.info("üéâ All tests PASSED! TensorRT deployment is ready.")
        return 0
    else:
        logger.error("‚ùå Some tests FAILED. Please check the errors above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
