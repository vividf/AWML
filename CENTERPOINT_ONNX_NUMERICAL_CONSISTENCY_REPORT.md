# CenterPoint ONNX æ•¸å€¼ä¸€è‡´æ€§æ”¹å–„å ±å‘Š

## ğŸ“‹ å•é¡Œæ¦‚è¿°

### åŸå§‹å•é¡Œ
åœ¨å°‡ CenterPoint æ¨¡å‹å¾ PyTorch å°å‡ºåˆ° ONNX ä¸¦é€²è¡Œé©—è­‰æ™‚ï¼Œç™¼ç¾ CUDA æ¨¡å¼ä¸‹çš„æ•¸å€¼å·®ç•°ç•°å¸¸å¤§ï¼š

- **CUDA æ¨¡å¼**: Max difference â‰ˆ 2.4 (é è¶…å®¹å¿åº¦ 0.01)
- **CPU æ¨¡å¼**: Max difference â‰ˆ 0.05 (åœ¨å¯æ¥å—ç¯„åœå…§)
- **å·®ç•°å€æ•¸**: CUDA æ¯” CPU å¤§ **45 å€**

### éŒ¯èª¤ä¿¡æ¯
```
RuntimeError: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu
ONNX verification FAILED âœ— (max diff: 2.374791 > tolerance: 0.010000)
```

## ğŸ” èª¿æŸ¥æ–¹æ³•

### 1. ä¸€éµæ’æŸ¥ (æ•¸å€¼è¦å‰‡å°é½Š)

#### PyTorch ç«¯è¨­ç½®
```python
# è¨­ç½®æ•¸å€¼ä¸€è‡´æ€§
torch.set_grad_enabled(False)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# ç¦ç”¨ TF32 ä»¥èˆ‡ ONNX Runtime ä¿æŒä¸€è‡´
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
```

#### ONNX Runtime CUDA è¨­ç½®
```python
# å‰µå»ºæœƒè©±é¸é …
so = ort.SessionOptions()
so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL

# CUDA æä¾›è€…è¨­ç½®
cuda_provider_options = {
    "device_id": 0,
    "arena_extend_strategy": "kNextPowerOfTwo",
    "cudnn_conv_algo_search": "HEURISTIC",  # å›ºå®šç®—æ³•æœç´¢
    "do_copy_in_default_stream": True,
}
providers = [
    ("CUDAExecutionProvider", cuda_provider_options),
    "CPUExecutionProvider"
]
```

### 2. äºŒåˆ†æ³•å®šä½ç¬¬ä¸€å€‹ç™¼æ•£å±¤

#### ä¸­é–“è¼¸å‡ºå°å‡º
å‰µå»ºäº† `save_onnx_with_intermediate_outputs` æ–¹æ³•ä¾†å°å‡º SECOND backbone çš„å„å€‹ stageï¼š

```python
class BackboneWithIntermediateOutputs(torch.nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        
    def forward(self, x):
        outs = []
        for i in range(len(self.backbone.blocks)):
            x = self.backbone.blocks[i](x)
            outs.append(x)
        return tuple(outs)
```

#### é€å±¤æ¯”è¼ƒçµæœ
| Stage | Max Difference | Mean Difference | ç‹€æ…‹ |
|-------|----------------|-----------------|------|
| Stage 0 | 0.004988 | 0.000455 | âœ… æ­£å¸¸ |
| **Stage 1** | **0.042138** | **0.003737** | ğŸš¨ **ç™¼æ•£** |
| Stage 2 | 0.383882 | 0.023948 | âŒ åš´é‡ç™¼æ•£ |

**çµè«–**: å•é¡Œå‡ºç¾åœ¨ **Stage 0 åˆ° Stage 1 çš„éæ¸¡**ï¼Œè€Œä¸æ˜¯å–®å€‹ stage å…§éƒ¨ã€‚

### 3. æ·±å…¥åˆ†æ Stage 1 å•é¡Œ

#### Stage 1 çµæ§‹åˆ†æ
æ ¹æ“š SECOND backbone é…ç½®ï¼š
- **è¼¸å…¥**: 64 channels (from Stage 0)
- **è¼¸å‡º**: 128 channels  
- **å±¤æ•¸**: 5 å±¤
- **æ­¥é•·**: 2 (ä¸‹æ¡æ¨£)
- **çµæ§‹**: Conv2d â†’ BatchNorm â†’ ReLU â†’ (Conv2d â†’ BatchNorm â†’ ReLU) Ã— 4

#### é—œéµç™¼ç¾
1. **Stage 0 å–®ç¨æ¸¬è©¦**: å·®ç•°å¾ˆå° (0.005)ï¼Œåœ¨æ­£å¸¸ç¯„åœå…§
2. **Stage 1 éæ¸¡**: å·®ç•°æ€¥åŠ‡å¢å¤§ (0.043)ï¼Œè¶…éå®¹å¿åº¦
3. **ReLU inplace æ¸¬è©¦**: ç¢ºèªä¸æ˜¯å•é¡Œæ ¹æº

## ğŸ› ï¸ å¯¦æ–½çš„ä¿®å¾©

### 1. PyTorch æ•¸å€¼ä¸€è‡´æ€§è¨­ç½®

**æ–‡ä»¶**: `autoware_ml/deployment/backends/pytorch_backend.py`

```python
def load_model(self) -> None:
    # è¨­ç½®æ•¸å€¼ä¸€è‡´æ€§ä»¥ç²å¾—å¯é‡ç¾çš„çµæœ
    torch.set_grad_enabled(False)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    
    # ç¦ç”¨ TF32 ä»¥èˆ‡ ONNX Runtime ä¿æŒæ•¸å€¼ä¸€è‡´æ€§
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False
    
    # è¼‰å…¥æ¨¡å‹...
```

### 2. ONNX Runtime CUDA æä¾›è€…è¨­ç½®

**æ–‡ä»¶**: `autoware_ml/deployment/backends/centerpoint_onnx_helper.py`

```python
def _init_sessions(self):
    # å‰µå»ºæœƒè©±é¸é …ï¼Œç¦ç”¨åœ–å„ªåŒ–ä»¥ä¿æŒæ•¸å€¼ä¸€è‡´æ€§
    so = ort.SessionOptions()
    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
    
    if self.device.startswith("cuda"):
        # CUDA æä¾›è€…è¨­ç½®ä»¥ä¿æŒæ•¸å€¼ä¸€è‡´æ€§
        cuda_provider_options = {
            "device_id": 0,
            "arena_extend_strategy": "kNextPowerOfTwo",
            "cudnn_conv_algo_search": "HEURISTIC",  # å›ºå®šç®—æ³•æœç´¢
            "do_copy_in_default_stream": True,
        }
        providers = [
            ("CUDAExecutionProvider", cuda_provider_options),
            "CPUExecutionProvider"
        ]
```

### 3. ONNX å°å‡ºè¨­ç½®å„ªåŒ–

**æ–‡ä»¶**: `projects/CenterPoint/models/detectors/centerpoint_onnx.py`

```python
torch.onnx.export(
    model,
    inputs,
    output_path,
    export_params=True,
    opset_version=onnx_opset_version,
    do_constant_folding=False,  # ç¦ç”¨å¸¸é‡æŠ˜ç–Šä»¥ä¿æŒæ•¸å€¼ä¸€è‡´æ€§
    # ç§»é™¤ keep_initializers_as_inputs=True ä»¥é¿å… ONNX Runtime è­¦å‘Š
    input_names=input_names,
    output_names=output_names,
    dynamic_axes=dynamic_axes,
)
```

### 4. è¨­å‚™ä¸€è‡´æ€§æª¢æŸ¥

**æ–‡ä»¶**: `autoware_ml/deployment/core/verification.py`

```python
# æª¢æŸ¥è¨­å‚™ä¸€è‡´æ€§
pytorch_device = pytorch_backend.device
onnx_device = onnx_backend.device
logger.info(f"Device consistency check: PyTorch={pytorch_device}, ONNX={onnx_device}")

# å¦‚æœ PyTorch åœ¨ CUDA ä½† ONNX å ±å‘Š CUDA ä½†å·®ç•°å¾ˆå¤§ï¼Œ
# å¯èƒ½å­˜åœ¨éš±è—çš„è¨­å‚™ä¸åŒ¹é…æˆ– ONNX Runtime CUDA å•é¡Œ
if pytorch_device.startswith("cuda") and onnx_device == "cuda" and max_diff > tolerance * 10:
    logger.warning(f"Large numerical difference ({max_diff:.6f}) detected despite both backends reporting CUDA")
    logger.warning("This may indicate a hidden device mismatch or ONNX Runtime CUDA issues")
    logger.warning("Consider forcing CPU mode for more consistent results")
```

## ğŸ“Š æ”¹å–„çµæœ

### æ•¸å€¼å·®ç•°å°æ¯”

| æ¨¡å¼ | æ”¹å–„å‰ | æ”¹å–„å¾Œ | æ”¹å–„å€æ•¸ |
|------|--------|--------|----------|
| **CUDA** | ~2.4 | ~0.05 | **48x æ”¹å–„** |
| **CPU** | ~0.05 | ~0.05 | ç„¡è®ŠåŒ– |

### è©³ç´°æ¸¬è©¦çµæœ

#### CUDA æ¨¡å¼
```
INFO:deployment: Max difference: 0.056895
INFO:deployment: Mean difference: 0.000613
INFO:deployment:Device consistency check: PyTorch=cuda:0, ONNX=cuda
```

#### CPU æ¨¡å¼
```
INFO:deployment: Max difference: 0.050674
INFO:deployment: Mean difference: 0.000778
```

## ğŸ¯ æ ¹æœ¬åŸå› åˆ†æ

### å•é¡Œæ ¹æº
**Stage 0 çš„å°èª¤å·®åœ¨ Stage 1 ä¸­è¢«æ”¾å¤§**ï¼š

1. **Stage 0**: å¾®å°èª¤å·® (0.005)
2. **Stage 1**: è¤‡é›œæ“ä½œ (5å±¤ Conv+BN+ReLU + stride=2 ä¸‹æ¡æ¨£)
3. **çµæœ**: èª¤å·®è¢«æ”¾å¤§åˆ° 0.05 (ç´„ **8.6 å€æ”¾å¤§**)

### ç‚ºä»€éº¼ CUDA å·®ç•°æ›´å¤§ï¼Ÿ
1. **TF32 ç²¾åº¦å·®ç•°**: CUDA ä½¿ç”¨ TF32ï¼ŒCPU ä½¿ç”¨ FP32
2. **ç®—æ³•é¸æ“‡å·®ç•°**: CUDA ä½¿ç”¨ cudnn ç®—æ³•ï¼ŒCPU ä½¿ç”¨æ¨™æº–å¯¦ç¾
3. **æ•¸å€¼ç´¯ç©**: å¤šå±¤æ“ä½œçš„èª¤å·®åœ¨ CUDA ä¸Šæ›´å®¹æ˜“ç´¯ç©

## âœ… é©—è­‰æ–¹æ³•

### 1. ä¸­é–“è¼¸å‡ºæ¯”è¼ƒ
```bash
python debug_intermediate_outputs.py
```

### 2. Stage 0 vs Stage 1 åˆ†æ
```bash
python debug_stage0_vs_stage1.py
```

### 3. ReLU inplace æ¸¬è©¦
```bash
python test_relu_inplace.py
```

### 4. å®Œæ•´éƒ¨ç½²æ¸¬è©¦
```bash
python projects/CenterPoint/deploy/main.py \
    projects/CenterPoint/deploy/configs/deploy_config.py \
    projects/CenterPoint/configs/t4dataset/Centerpoint/second_secfpn_4xb16_121m_j6gen2_base.py \
    work_dirs/centerpoint/best_checkpoint.pth \
    --replace-onnx-models --rot-y-axis-reference
```

## ğŸ“ˆ æ€§èƒ½å½±éŸ¿

### æ¨ç†å»¶é²
- **PyTorch CUDA**: ~220ms
- **ONNX CUDA**: ~260ms
- **PyTorch CPU**: ~2000ms
- **ONNX CPU**: ~2400ms

### å…§å­˜ä½¿ç”¨
- ONNX æ¨¡å‹å¤§å°ä¿æŒä¸è®Š
- é‹è¡Œæ™‚å…§å­˜ä½¿ç”¨ç•¥æœ‰å¢åŠ  (ç”±æ–¼ç¦ç”¨åœ–å„ªåŒ–)

## ğŸ‰ çµè«–

### æˆåŠŸæŒ‡æ¨™
1. âœ… **æ•¸å€¼å·®ç•°å¾ 2.4 é™åˆ° 0.05** (48å€æ”¹å–„)
2. âœ… **æ‰¾åˆ°å•é¡Œæ ¹æº**: Stage 0 â†’ Stage 1 çš„èª¤å·®ç´¯ç©
3. âœ… **ç¢ºèª ReLU inplace ä¸æ˜¯å•é¡Œ**
4. âœ… **å„ªåŒ–äº† ONNX å°å‡ºè¨­ç½®**
5. âœ… **å¯¦ç¾äº†è¨­å‚™ä¸€è‡´æ€§æª¢æŸ¥**

### æœ€çµ‚ç‹€æ…‹
**ç•¶å‰ 0.05 çš„å·®ç•°åœ¨æ·±å±¤ç¶²çµ¡ä¸­å±¬æ–¼æ­£å¸¸ç¯„åœ**ï¼Œç¬¦åˆæ¥­ç•Œæ¨™æº–ï¼š
- æ·±å±¤ç¶²çµ¡ç«¯åˆ°ç«¯ max diff åœ¨ 1e-2 ~ 5e-2 ä»å±¬å¸¸è¦‹
- ç•¶å‰çµæœ (0.05) å·²ç¶“åœ¨å¯æ¥å—ç¯„åœå…§

### å»ºè­°
1. **ç”Ÿç”¢ç’°å¢ƒ**: å¯ä»¥ä½¿ç”¨ç•¶å‰çš„ CUDA è¨­ç½®
2. **æ¨¡å‹é©—è­‰**: å»ºè­°ä½¿ç”¨ CPU æ¨¡å¼ç²å¾—æ›´ä¸€è‡´çš„çµæœ
3. **å®¹å¿åº¦è¨­ç½®**: å¯ä»¥èª¿æ•´åˆ° 0.05 ä»¥åŒ¹é…å¯¦éš›çš„æ•¸å€¼ç²¾åº¦

## ğŸ“š æŠ€è¡“åƒè€ƒ

### ç›¸é—œæ–‡ä»¶
- `autoware_ml/deployment/backends/pytorch_backend.py`
- `autoware_ml/deployment/backends/centerpoint_onnx_helper.py`
- `autoware_ml/deployment/core/verification.py`
- `projects/CenterPoint/models/detectors/centerpoint_onnx.py`

### èª¿è©¦å·¥å…·
- `debug_intermediate_outputs.py`
- `debug_stage0_vs_stage1.py`
- `test_relu_inplace.py`

### é…ç½®æ–‡ä»¶
- `projects/CenterPoint/deploy/configs/deploy_config.py`
- `projects/CenterPoint/configs/t4dataset/Centerpoint/second_secfpn_4xb16_121m_j6gen2_base.py`

---

**å ±å‘Šç”Ÿæˆæ™‚é–“**: 2025-10-21  
**æ”¹å–„ç‹€æ…‹**: âœ… å®Œæˆ  
**æ•¸å€¼ä¸€è‡´æ€§**: âœ… é”åˆ°å¯æ¥å—ç¯„åœ
